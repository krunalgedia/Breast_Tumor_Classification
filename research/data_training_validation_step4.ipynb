{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e100450e-6284-4b96-b040-8861d00b2c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\kbged\\\\Downloads\\\\mlprojects\\\\Breast_Tumor_Classification\\\\research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf1a5d63-2bcc-443e-9c3a-a7ad225ff474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\kbged\\\\Downloads\\\\mlprojects\\\\Breast_Tumor_Classification'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e114830d-7bf6-42e7-ab07-4303713ded3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#d715d4e3ea26e0aad712816503f3e9a25a4ebc46\n",
    "\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"]=\"https://dagshub.com/krunalgedia/Breast_Tumor_Classification.mlflow\"\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"]=\"krunalgedia\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"]=\"d715d4e3ea26e0aad712816503f3e9a25a4ebc46\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ede3e70b-bd27-4621-8dd1-3e455fa176b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTrainingValidationConfig:\n",
    "    root_dir: Path\n",
    "    STATUS_FILE: str\n",
    "    unzip_dir: str\n",
    "    benign_dir: str\n",
    "    malignant_dir: str\n",
    "    normal_dir: str\n",
    "    benign: str\n",
    "    normal: str\n",
    "    malignant: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6579c00b-af91-4f88-99ee-df35ac88da63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tumorClassify.constants import *\n",
    "#from tumorClassify.utils.common import read_yaml, create_directories, compute_metrics, scale_bounding_box, create_bounding_box\n",
    "#from tumorClassify.utils.common import DocumentClassificationDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6387b044-69f8-48a4-8d5d-6bfbe29b8836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kbged\\Miniconda3\\envs\\tumormlflow\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kbged\\Miniconda3\\envs\\tumormlflow\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\kbged\\Miniconda3\\envs\\tumormlflow\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.15.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tumorClassify.utils.common import read_yaml, create_directories\n",
    "from tumorClassify.utils.classification_utils import TrainValEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e64e0252-e4f5-48b2-9897-8c0e67b925fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-14 14:02:54\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mcommon.py:30\u001b[0m | \u001b[1myaml file: src\\tumorClassify\\config\\config.yaml loaded successfully\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'artifacts/data_ingestion/data/benign'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_yaml(CONFIG_FILE_PATH).data_training_validation.benign_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9353548d-36a8-47a6-b7d4-c1c7686c700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self,\n",
    "                 config_filepath = CONFIG_FILE_PATH,\n",
    "                 params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_training_validation_config(self) -> DataTrainingValidationConfig:\n",
    "        \n",
    "        config = self.config.data_training_validation\n",
    "        print(config)\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_training_validation_config = DataTrainingValidationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            STATUS_FILE=config.STATUS_FILE,\n",
    "            unzip_dir=config.unzip_dir,\n",
    "            benign_dir=config.benign_dir,\n",
    "            malignant_dir=config.malignant_dir,\n",
    "            normal_dir=config.normal_dir,\n",
    "            benign = config.benign,\n",
    "            normal = config.normal,\n",
    "            malignant = config.malignant,\n",
    "            )\n",
    "\n",
    "        return data_training_validation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d1bd585-51eb-4e21-8f28-03df28a877cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tumorClassify import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c02d966b-e21c-4193-83f4-0f61e44b8c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-14 14:02:54\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mcommon.py:30\u001b[0m | \u001b[1myaml file: src\\tumorClassify\\config\\config.yaml loaded successfully\u001b[0m\n",
      "\u001b[32m2024-07-14 14:02:54\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mcommon.py:30\u001b[0m | \u001b[1myaml file: params.yaml loaded successfully\u001b[0m\n",
      "\u001b[32m2024-07-14 14:02:54\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mcommon.py:53\u001b[0m | \u001b[1mDirectory already exists at: artifacts\u001b[0m\n",
      "{'root_dir': 'artifacts/data_training_validation', 'STATUS_FILE': 'artifacts/data_training_validation/status.txt', 'unzip_dir': 'artifacts/data_ingestion/data', 'benign_dir': 'artifacts/data_ingestion/data/benign', 'malignant_dir': 'artifacts/data_ingestion/data/malignant', 'normal_dir': 'artifacts/data_ingestion/data/normal', 'benign': 'benign', 'malignant': 'malignant', 'normal': 'normal'}\n",
      "\u001b[32m2024-07-14 14:02:54\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mcommon.py:53\u001b[0m | \u001b[1mDirectory already exists at: artifacts/data_training_validation\u001b[0m\n",
      "64 tf.data.AUTOTUNE\n",
      "1 tf.data.AUTOTUNE\n",
      "WARNING:tensorflow:From C:\\Users\\kbged\\Miniconda3\\envs\\tumormlflow\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\kbged\\Miniconda3\\envs\\tumormlflow\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = ConfigurationManager()\n",
    "data_training_validation_config = config.get_data_training_validation_config()\n",
    "\n",
    "trainValEval = TrainValEval(config.config, config.params)\n",
    "\n",
    "x_benign, y_benign, y_benign_label = trainValEval.load_data(data_training_validation_config.benign)\n",
    "x_malignant, y_malignant, y_malignant_label = trainValEval.load_data(data_training_validation_config.malignant)\n",
    "x_normal, y_normal, y_normal_label = trainValEval.load_data(data_training_validation_config.normal)\n",
    "\n",
    "trainValEval.prepare_dataset(x_benign, x_malignant, x_normal, y_benign_label, y_malignant_label, y_normal_label)\n",
    "trainValEval.prepare_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bd0c0c9-991c-4b31-a9c8-c32723a98d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kbged\\Miniconda3\\envs\\tumormlflow\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\kbged\\Miniconda3\\envs\\tumormlflow\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "10/10 [==============================] - ETA: 0s - loss: 6.5008 - recall_c0: 0.3612 - recall_c1: 0.3389 - recall_c2: 0.3464 - precision_c0: 0.1816 - precision_c1: 0.5463 - precision_c2: 0.3002 \n",
      "Epoch 1: val_loss improved from inf to 7.18100, saving model to VGG19weights.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kbged\\Miniconda3\\envs\\tumormlflow\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 299s 30s/step - loss: 6.5008 - recall_c0: 0.3612 - recall_c1: 0.3389 - recall_c2: 0.3464 - precision_c0: 0.1816 - precision_c1: 0.5463 - precision_c2: 0.3002 - val_loss: 7.1810 - val_recall_c0: 0.1218 - val_recall_c1: 0.2115 - val_recall_c2: 0.0513 - val_precision_c0: 0.1218 - val_precision_c1: 0.2115 - val_precision_c2: 0.0513 - lr: 2.0000e-04\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\kbged\\AppData\\Local\\Temp\\tmpdwe2es7_\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\kbged\\AppData\\Local\\Temp\\tmpdwe2es7_\\model\\data\\model\\assets\n",
      "2024/07/14 14:09:15 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\kbged\\AppData\\Local\\Temp\\tmpdwe2es7_\\model, flavor: tensorflow). Fall back to return ['tensorflow==2.15.0', 'cloudpickle==3.0.0']. Set logging level to DEBUG to see the full traceback. \n",
      "2024/07/14 14:09:15 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"C:\\Users\\kbged\\Miniconda3\\envs\\tumormlflow\\lib\\site-packages\\_distutils_hack\\__init__.py:32: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\"\n",
      "2024/07/14 14:10:38 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\kbged\\AppData\\Local\\Temp\\tmpe7miu5ky\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\kbged\\AppData\\Local\\Temp\\tmpe7miu5ky\\model\\data\\model\\assets\n",
      "2024/07/14 14:11:18 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\kbged\\AppData\\Local\\Temp\\tmpe7miu5ky\\model, flavor: tensorflow). Fall back to return ['tensorflow==2.15.0', 'cloudpickle==3.0.0']. Set logging level to DEBUG to see the full traceback. \n",
      "Successfully registered model 'VGG19'.\n",
      "2024/07/14 14:12:35 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: VGG19, version 1\n",
      "Created version '1' of model 'VGG19'.\n"
     ]
    }
   ],
   "source": [
    "trainValEval.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf41cd98-47af-4726-bd81-f57b96ce16d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a98ca42-dae7-4adf-8352-aeed4f508619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4c8e1e-4cb2-41cf-b280-297bf820608a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ac5b94-14cc-47e5-b5c4-a820fd008d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1901eed-84f7-4914-a45a-48de88c8d1b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6a1478a-f971-4a2e-8ee6-47ce35fc817a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss [6.234276294708252, 5.808884620666504]\n",
      "recall_c0 [0.2055555284023285, 0.40940171480178833]\n",
      "recall_c1 [0.4064103364944458, 0.45662397146224976]\n",
      "recall_c2 [0.24871791899204254, 0.414743572473526]\n",
      "precision_c0 [0.1707264930009842, 0.2589743435382843]\n",
      "precision_c1 [0.6183761358261108, 0.7745727300643921]\n",
      "precision_c2 [0.2350427210330963, 0.3741452991962433]\n",
      "val_loss [5.746504783630371, 5.556178569793701]\n",
      "val_recall_c0 [0.06410256028175354, 0.08974358439445496]\n",
      "val_recall_c1 [0.4358973801136017, 0.3205128014087677]\n",
      "val_recall_c2 [0.05769230052828789, 0.12179485708475113]\n",
      "val_precision_c0 [0.06410256028175354, 0.08974358439445496]\n",
      "val_precision_c1 [0.4358973801136017, 0.3205128014087677]\n",
      "val_precision_c2 [0.05769230052828789, 0.12179485708475113]\n",
      "lr [0.0002, 0.0002]\n"
     ]
    }
   ],
   "source": [
    "m = {'loss': [6.234276294708252, 5.808884620666504], 'recall_c0': [0.2055555284023285, 0.40940171480178833], 'recall_c1': [0.4064103364944458, 0.45662397146224976], 'recall_c2': [0.24871791899204254, 0.414743572473526], 'precision_c0': [0.1707264930009842, 0.2589743435382843], 'precision_c1': [0.6183761358261108, 0.7745727300643921], 'precision_c2': [0.2350427210330963, 0.3741452991962433], 'val_loss': [5.746504783630371, 5.556178569793701], 'val_recall_c0': [0.06410256028175354, 0.08974358439445496], 'val_recall_c1': [0.4358973801136017, 0.3205128014087677], 'val_recall_c2': [0.05769230052828789, 0.12179485708475113], 'val_precision_c0': [0.06410256028175354, 0.08974358439445496], 'val_precision_c1': [0.4358973801136017, 0.3205128014087677], 'val_precision_c2': [0.05769230052828789, 0.12179485708475113], 'lr': [0.0002, 0.0002]}\n",
    "for k,v in m.items():\n",
    "    print(k,v)\n",
    "    #for key, value in logs.items():\n",
    "    #    mlflow.log_metric(key, value, step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f32a6d7-0b78-435c-89df-04135415bde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 6.234276294708252 0\n",
      "recall_c0 0.2055555284023285 0\n",
      "recall_c1 0.4064103364944458 0\n",
      "recall_c2 0.24871791899204254 0\n",
      "precision_c0 0.1707264930009842 0\n",
      "precision_c1 0.6183761358261108 0\n",
      "precision_c2 0.2350427210330963 0\n",
      "val_loss 5.746504783630371 0\n",
      "val_recall_c0 0.06410256028175354 0\n",
      "val_recall_c1 0.4358973801136017 0\n",
      "val_recall_c2 0.05769230052828789 0\n",
      "val_precision_c0 0.06410256028175354 0\n",
      "val_precision_c1 0.4358973801136017 0\n",
      "val_precision_c2 0.05769230052828789 0\n",
      "lr 0.0002 0\n",
      "loss 5.808884620666504 1\n",
      "recall_c0 0.40940171480178833 1\n",
      "recall_c1 0.45662397146224976 1\n",
      "recall_c2 0.414743572473526 1\n",
      "precision_c0 0.2589743435382843 1\n",
      "precision_c1 0.7745727300643921 1\n",
      "precision_c2 0.3741452991962433 1\n",
      "val_loss 5.556178569793701 1\n",
      "val_recall_c0 0.08974358439445496 1\n",
      "val_recall_c1 0.3205128014087677 1\n",
      "val_recall_c2 0.12179485708475113 1\n",
      "val_precision_c0 0.08974358439445496 1\n",
      "val_precision_c1 0.3205128014087677 1\n",
      "val_precision_c2 0.12179485708475113 1\n",
      "lr 0.0002 1\n"
     ]
    }
   ],
   "source": [
    "# Log metrics for each epoch\n",
    "metrics = m.keys()\n",
    "for epoch in range(len(m[list(metrics)[0]])):\n",
    "    for metric in metrics:\n",
    "        print(metric, m[metric][epoch], epoch)\n",
    "        #mlflow.log_metric(metric, history.history[metric][epoch], step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f2a50f-fe29-42a5-8138-5f844aa383aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log metrics for each epoch\n",
    "for epoch, logs in enumerate(history.history):\n",
    "    for key, value in logs.items():\n",
    "        mlflow.log_metric(key, value, step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a70d81-9c6b-44a9-9a86-91fb96e67ed4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90d3b83f-325f-40f1-bd1e-b281a65a3310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-13 22:46:48\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mcommon.py:30\u001b[0m | \u001b[1myaml file: src\\tumorClassify\\config\\config.yaml loaded successfully\u001b[0m\n",
      "\u001b[32m2024-07-13 22:46:48\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mcommon.py:30\u001b[0m | \u001b[1myaml file: params.yaml loaded successfully\u001b[0m\n",
      "\u001b[32m2024-07-13 22:46:48\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mcommon.py:53\u001b[0m | \u001b[1mDirectory already exists at: artifacts\u001b[0m\n",
      "{'root_dir': 'artifacts/data_training_validation', 'STATUS_FILE': 'artifacts/data_training_validation/status.txt', 'unzip_dir': 'artifacts/data_ingestion/data', 'benign_dir': 'artifacts/data_ingestion/data/benign', 'malignant_dir': 'artifacts/data_ingestion/data/malignant', 'normal_dir': 'artifacts/data_ingestion/data/normal', 'benign': 'benign', 'malignant': 'malignant', 'normal': 'normal'}\n",
      "\u001b[32m2024-07-13 22:46:48\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mcommon.py:53\u001b[0m | \u001b[1mDirectory already exists at: artifacts/data_training_validation\u001b[0m\n"
     ]
    },
    {
     "ename": "BoxKeyError",
     "evalue": "\"'ConfigBox' object has no attribute 'unzip_dir'\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\Miniconda3\\envs\\tumormlflow\\lib\\site-packages\\box\\box.py:503\u001b[0m, in \u001b[0;36mbox.box.Box.__getitem__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'unzip_dir'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mBoxKeyError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\Miniconda3\\envs\\tumormlflow\\lib\\site-packages\\box\\box.py:536\u001b[0m, in \u001b[0;36mbox.box.Box.__getattr__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\tumormlflow\\lib\\site-packages\\box\\box.py:524\u001b[0m, in \u001b[0;36mbox.box.Box.__getitem__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mBoxKeyError\u001b[0m: \"'unzip_dir'\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\Miniconda3\\envs\\tumormlflow\\lib\\site-packages\\box\\box.py:538\u001b[0m, in \u001b[0;36mbox.box.Box.__getattr__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ConfigBox' object has no attribute 'unzip_dir'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mBoxKeyError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\Miniconda3\\envs\\tumormlflow\\lib\\site-packages\\box\\config_box.py:28\u001b[0m, in \u001b[0;36mbox.config_box.ConfigBox.__getattr__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\tumormlflow\\lib\\site-packages\\box\\box.py:552\u001b[0m, in \u001b[0;36mbox.box.Box.__getattr__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mBoxKeyError\u001b[0m: \"'ConfigBox' object has no attribute 'unzip_dir'\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\Miniconda3\\envs\\tumormlflow\\lib\\site-packages\\box\\box.py:503\u001b[0m, in \u001b[0;36mbox.box.Box.__getitem__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'unzip_dir'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mBoxKeyError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\Miniconda3\\envs\\tumormlflow\\lib\\site-packages\\box\\box.py:536\u001b[0m, in \u001b[0;36mbox.box.Box.__getattr__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\tumormlflow\\lib\\site-packages\\box\\box.py:524\u001b[0m, in \u001b[0;36mbox.box.Box.__getitem__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mBoxKeyError\u001b[0m: \"'unzip_dir'\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\Miniconda3\\envs\\tumormlflow\\lib\\site-packages\\box\\box.py:538\u001b[0m, in \u001b[0;36mbox.box.Box.__getattr__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ConfigBox' object has no attribute 'unzip_dir'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mBoxKeyError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m config \u001b[38;5;241m=\u001b[39m ConfigurationManager()\n\u001b[0;32m      2\u001b[0m data_training_validation_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_data_training_validation_config()\n\u001b[1;32m----> 5\u001b[0m TrainValEval \u001b[38;5;241m=\u001b[39m \u001b[43mTrainValEval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m TrainValEval\u001b[38;5;241m.\u001b[39mload_data(config\u001b[38;5;241m.\u001b[39mbenign)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#model = 'ResNet152V2'\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#model = 'VGG19'\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#model = 'EfficientNetB7'\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#model = 'EfficientNetV2S'\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#model = 'ConvNeXtBase'\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#model = 'vit_b16'\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\kbged\\downloads\\mlprojects\\breast_tumor_classification\\src\\tumorClassify\\utils\\classification_utils.py:12\u001b[0m, in \u001b[0;36mTrainValEval.__init__\u001b[1;34m(self, config, param)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam \u001b[38;5;241m=\u001b[39m param\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munzip_dir\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam\u001b[38;5;241m.\u001b[39mimage_params\u001b[38;5;241m.\u001b[39mSIZE\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam\u001b[38;5;241m.\u001b[39mmodel_params\u001b[38;5;241m.\u001b[39mMODEL\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\tumormlflow\\lib\\site-packages\\box\\config_box.py:30\u001b[0m, in \u001b[0;36mbox.config_box.ConfigBox.__getattr__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\tumormlflow\\lib\\site-packages\\box\\box.py:552\u001b[0m, in \u001b[0;36mbox.box.Box.__getattr__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mBoxKeyError\u001b[0m: \"'ConfigBox' object has no attribute 'unzip_dir'\""
     ]
    }
   ],
   "source": [
    "config = ConfigurationManager()\n",
    "data_training_validation_config = config.get_data_training_validation_config()\n",
    "\n",
    "\n",
    "TrainValEval = TrainValEval(config.config, config.params)\n",
    "TrainValEval.load_data(config.benign)\n",
    "\n",
    "#model = 'ResNet152V2'\n",
    "#model = 'VGG19'\n",
    "#model = 'EfficientNetB7'\n",
    "#model = 'EfficientNetV2S'\n",
    "#model = 'ConvNeXtBase'\n",
    "#model = 'vit_b16'\n",
    "\n",
    "x_benign, y_benign, y_benign_label = TrainValEval.load_data(config.benign)\n",
    "x_malignant, y_malignant, y_malignant_label = TrainValEval.load_data(config.malignant)\n",
    "x_normal, y_normal, y_normal_label = TrainValEval.load_data(config.normal)\n",
    "\n",
    "#load_data(config.DATA_DIR, config.SIZE, config.benign, model )\n",
    "#x_malignant, y_malignant = load_data(config.DATA_DIR, config.SIZE, config.malignant, model)\n",
    "#x_normal, y_normal = load_data(config.DATA_DIR, config.SIZE, config.normal, model)\n",
    "\n",
    "if config.DEPTH==1:\n",
    "  # prepare data to modeling\n",
    "  x_benign = np.expand_dims(x_benign, -1)\n",
    "  y_benign = np.expand_dims(y_benign, -1)\n",
    "\n",
    "  # prepare data to modeling\n",
    "  x_malignant = np.expand_dims(x_malignant, -1)\n",
    "  y_malignant = np.expand_dims(y_malignant, -1)\n",
    "\n",
    "  # prepare data to modeling\n",
    "  x_normal = np.expand_dims(x_normal, -1)\n",
    "  y_normal = np.expand_dims(y_normal, -1)\n",
    "\n",
    "y_normal_label    = np.array([[1,0,0] for i in range(len(y_normal))])\n",
    "y_benign_label    = np.array([[0,1,0] for i in range(len(y_benign))])\n",
    "y_malignant_label = np.array([[0,0,1] for i in range(len(y_malignant))])\n",
    "\n",
    "x_total_label = np.concatenate((x_benign, x_malignant, x_normal), axis=0)\n",
    "y_total_label = np.concatenate((y_benign_label, y_malignant_label, y_normal_label), axis=0)\n",
    "random_state = 24\n",
    "y_total_label = np.expand_dims(y_total_label, -1)\n",
    "\n",
    "X_train_total_label, X_test_total_label, y_train_total_label, y_test_total_label = train_test_split(\n",
    "                                                                                   x_total_label,\n",
    "                                                                                   y_total_label,\n",
    "                                                                                   test_size=config.TEST_SIZE,\n",
    "                                                                                   shuffle=config.SHUFFLE,\n",
    "                                                                                   random_state=config.SEED)\n",
    "\n",
    "\n",
    "#y_malignant = np.expand_dims(y_malignant, -1)\n",
    "y_train_total_label = np.reshape(y_train_total_label,(-1,3))\n",
    "y_test_total_label = np.reshape(y_test_total_label,(-1,3))\n",
    "\n",
    "ny = len(y_benign_label) + len(y_malignant_label) + len(y_normal_label)\n",
    "n_classes = 3\n",
    "class_weight = {0: ny/(n_classes*len(y_normal_label)),\n",
    "                1: ny/(n_classes*len(y_benign_label)),\n",
    "                2: ny/(n_classes*len(y_malignant_label))}\n",
    "\n",
    "ds_train_label = tf.data.Dataset.from_tensor_slices((X_train_total_label, y_train_total_label))\n",
    "ds_test_label  = tf.data.Dataset.from_tensor_slices((X_test_total_label, y_test_total_label))\n",
    "\n",
    "\n",
    "def prepare_dataset(x,y,batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    dataset = dataset.batch(batch_size).cache().prefetch(buffer_size=config.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "dataset_train_label = prepare_dataset(X_train_total_label, y_train_total_label, config.BATCH_SIZE)\n",
    "#dataset_val = prepare_dataset(val_df_scaled,62)\n",
    "dataset_test_label = prepare_dataset(X_test_total_label,y_test_total_label,1)\n",
    "\n",
    "\n",
    "POOLING = 'max'\n",
    "\n",
    "# Input layer\n",
    "input_layer = tf.keras.layers.Input(shape=(224, 224, 3))\n",
    "\n",
    "if model=='ResNet152V2':\n",
    "  pretrained_model = tf.keras.applications.ResNet152V2(\n",
    "    include_top=False,\n",
    "    input_shape=(config.SIZE, config.SIZE, config.DEPTH),\n",
    "    weights=\"imagenet\",\n",
    "    pooling=POOLING,\n",
    "    )(input_layer)\n",
    "  pretrained_model.trainable=False\n",
    "elif model=='VGG19':\n",
    "  pretrained_model = tf.keras.applications.VGG19(\n",
    "    include_top=False,\n",
    "    input_shape=(config.SIZE, config.SIZE, config.DEPTH),\n",
    "    pooling=POOLING,\n",
    "    weights='imagenet'\n",
    "    )(input_layer)\n",
    "elif model=='EfficientNetB7':\n",
    "  pretrained_model = tf.keras.applications.EfficientNetB7(\n",
    "    include_top=False,\n",
    "    input_shape=(config.SIZE, config.SIZE, config.DEPTH),\n",
    "    weights=\"imagenet\",\n",
    "    pooling=POOLING,\n",
    "    )(input_layer)\n",
    "elif model=='EfficientNetV2S':\n",
    "  pretrained_model = tf.keras.applications.EfficientNetV2S(\n",
    "    include_top=False,\n",
    "    input_shape=(config.SIZE, config.SIZE, config.DEPTH),\n",
    "    weights=\"imagenet\",\n",
    "    pooling=POOLING,\n",
    "    )(input_layer)\n",
    "elif model=='ConvNeXtBase':\n",
    "  pretrained_model = tf.keras.applications.ConvNeXtBase(\n",
    "    model_name=\"convnext_base\",\n",
    "    include_top=False,\n",
    "    include_preprocessing=True,\n",
    "    weights=\"imagenet\",\n",
    "    pooling=POOLING,\n",
    "    )(input_layer)\n",
    "elif model=='vit_b16':\n",
    "  pretrained_model = vit.vit_b16(\n",
    "    #image_size=image_size,\n",
    "    #activation='sigmoid',\n",
    "    pretrained = True,\n",
    "    include_top = False,\n",
    "    pretrained_top = False,\n",
    "    )(input_layer)\n",
    "\n",
    "# Flatten layer\n",
    "flattened = tf.keras.layers.Flatten()(pretrained_model)\n",
    "\n",
    "# Dense layers\n",
    "#dense1 = tf.keras.layers.Dense(256, activation='relu',\n",
    "              #kernel_regularizer=regularizers.l2(0.01),  # L2 regularization\n",
    "              #bias_regularizer=regularizers.l2(0.01))(flattened)\n",
    "#batch_norm1 = tf.keras.layers.BatchNormalization()(dense1)\n",
    "#dropout1 = tf.keras.layers.Dropout(0.5)(batch_norm1)\n",
    "dense1 = tf.keras.layers.Dense(256, activation='relu',\n",
    "              kernel_regularizer=regularizers.l2(0.01),  # L2 regularization\n",
    "              bias_regularizer=regularizers.l2(0.01))(flattened)\n",
    "batch_norm1 = tf.keras.layers.BatchNormalization()(dense1)\n",
    "dropout1 = tf.keras.layers.Dropout(0.3)(batch_norm1)\n",
    "dense2 = tf.keras.layers.Dense(64, activation='relu',\n",
    "              kernel_regularizer=regularizers.l2(0.01),  # L2 regularization\n",
    "              bias_regularizer=regularizers.l2(0.01),)(dropout1)\n",
    "batch_norm2 = tf.keras.layers.BatchNormalization()(dense2)\n",
    "dropout2 = tf.keras.layers.Dropout(0.2)(batch_norm2)\n",
    "output_layer = tf.keras.layers.Dense(3, activation='softmax')(dropout2)\n",
    "\n",
    "\n",
    "# Functional model\n",
    "fine_tune_model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "fine_tune_model.layers[1].trainable=False\n",
    "pretrained_model.trainable=False\n",
    "\n",
    "def recall(y_true, y_pred, c):\n",
    "    pred_labels = K.cast(K.argmax(y_pred, axis=-1), K.floatx())\n",
    "    true_labels = K.cast(K.argmax(y_true, axis=-1), K.floatx())\n",
    "\n",
    "    tp = K.sum(K.cast(tf.logical_and(true_labels == c, pred_labels == c),K.floatx()))\n",
    "    fn = K.sum(K.cast(tf.logical_and(true_labels == c, pred_labels != c),K.floatx()))\n",
    "    tn = K.sum(K.cast(tf.logical_and(true_labels != c, pred_labels != c),K.floatx()))\n",
    "    fp = K.sum(K.cast(tf.logical_and(true_labels != c, pred_labels == c),K.floatx()))\n",
    "\n",
    "    sensitivity = (tp)/(tp+fn+0.0000001)\n",
    "    return sensitivity\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred, c):\n",
    "    pred_labels = K.cast(K.argmax(y_pred, axis=-1), K.floatx())\n",
    "    true_labels = K.cast(K.argmax(y_true, axis=-1), K.floatx())\n",
    "\n",
    "    tp = K.sum(K.cast(tf.logical_and(true_labels == c, pred_labels == c),K.floatx()))\n",
    "    fn = K.sum(K.cast(tf.logical_and(true_labels == c, pred_labels != c),K.floatx()))\n",
    "    tn = K.sum(K.cast(tf.logical_and(true_labels != c, pred_labels != c),K.floatx()))\n",
    "    fp = K.sum(K.cast(tf.logical_and(true_labels != c, pred_labels == c),K.floatx()))\n",
    "\n",
    "    precision = (tp)/(tp+fp+0.0000001)\n",
    "    return precision\n",
    "\n",
    "def recall_c0(y_true, y_pred):\n",
    "    return recall(y_true, y_pred, 0)\n",
    "\n",
    "def precision_c0(y_true, y_pred):\n",
    "    return precision(y_true, y_pred, 0)\n",
    "\n",
    "def recall_c1(y_true, y_pred):\n",
    "    return recall(y_true, y_pred, 1)\n",
    "\n",
    "def precision_c1(y_true, y_pred):\n",
    "    return precision(y_true, y_pred, 1)\n",
    "\n",
    "def recall_c2(y_true, y_pred):\n",
    "    return recall(y_true, y_pred, 2)\n",
    "\n",
    "def precision_c2(y_true, y_pred):\n",
    "    return precision(y_true, y_pred, 2)\n",
    "\n",
    "\n",
    "\n",
    "METRICS = [\n",
    "      #tf.keras.metrics.BinaryCrossentropy(name='cross entropy'),  # same as model's loss\n",
    "      #tf.keras.metrics.MeanSquaredError(name='Brier score'),\n",
    "      tf.keras.metrics.TruePositives(name='tp'),\n",
    "      tf.keras.metrics.FalsePositives(name='fp'),\n",
    "      tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "      tf.keras.metrics.FalseNegatives(name='fn'),\n",
    "      #tf.keras.metrics.Accuracy(name='accuracy'),\n",
    "      #tf.keras.metrics.Precision(name='precision'),\n",
    "      #tf.keras.metrics.Recall(name='recall'),\n",
    "      #tf.keras.metrics.AUC(name='auc'),\n",
    "      #tf.keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]\n",
    "\n",
    "fine_tune_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002),\n",
    "              #loss=dice_coef_loss,\n",
    "              #loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "              loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    "              #metrics = [metric_meaniou(),tf.keras.metrics.BinaryIoU(name='IoU')],\n",
    "              metrics = [recall_c0,recall_c1,recall_c2,\n",
    "                         precision_c0,precision_c1,precision_c2],\n",
    "              #metrics = [recall_c2],\n",
    "              #run_eagerly=True\n",
    "              )\n",
    "\n",
    "#config.NUM_EPOCHS=1\n",
    "checkpoint = ModelCheckpoint(model+'weights.hdf5' ,\n",
    "                             monitor = 'val_loss',\n",
    "                             verbose = 1,\n",
    "                             save_best_only=True,\n",
    "                             mode = 'min',\n",
    "                             save_weights_only=True,\n",
    "                             save_freq='epoch'\n",
    "                            )\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3,\n",
    "                              patience=15, min_lr=0.00005)\n",
    "\n",
    "callbacks_list = [checkpoint, reduce_lr]\n",
    "\n",
    "# Fit model\n",
    "history = fine_tune_model.fit(dataset_train_label,\n",
    "                    validation_data=dataset_test_label,\n",
    "                    #steps_per_epoch=len(X_train)/(6),\n",
    "                    #validation_steps=10,\n",
    "                    callbacks=callbacks_list,\n",
    "                    epochs=config.NUM_EPOCHS,\n",
    "                    class_weight=class_weight,\n",
    "                    #verbose=2,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6c03d3d-1f33-48f7-b611-74925a57e3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "import json\n",
    "from PIL import Image\n",
    "from transformers import LayoutLMv3FeatureExtractor, LayoutLMv3TokenizerFast, LayoutLMv3Processor, LayoutLMv3ForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class TrainAndValidate:\n",
    "    def __init__(self, config: DataTrainingValidationConfig):\n",
    "        self.feature_extractor = LayoutLMv3FeatureExtractor(apply_ocr=False)\n",
    "        self.tokenizer = LayoutLMv3TokenizerFast.from_pretrained(\"microsoft/layoutlmv3-base\")\n",
    "        self.processor = LayoutLMv3Processor(self.feature_extractor, self.tokenizer)\n",
    "        self.config = config\n",
    "        self.DOCUMENT_CLASSES = sorted(list(map(lambda p: p.name,Path(self.config.unzip_dir).glob(\"*\"))))    \n",
    "\n",
    "    def get_train_test_path(self):\n",
    "        # Convert PosixPath objects to strings\n",
    "        image_paths=sorted(list(Path(self.config.unzip_dir).glob(\"*/*.png\")))\n",
    "        image_paths_str = [str(path) for path in image_paths]\n",
    "        \n",
    "        # Define labels based on whether the paths contain specific strings\n",
    "        income_labels = [\"income\" in path for path in image_paths_str]\n",
    "        balance_labels = [\"balance\" in path for path in image_paths_str]\n",
    "        cashflow_labels = [\"cashflow\" in path for path in image_paths_str]\n",
    "        \n",
    "        # Use any one of the labels as the target for stratified split\n",
    "        # Here, I'm using income_labels, but you can choose based on your requirements\n",
    "        train_images_str, test_images_str = train_test_split(image_paths_str, test_size=0.2, stratify=income_labels, random_state=42)\n",
    "        \n",
    "        # Convert back to PosixPath objects\n",
    "        train_images = [Path(path) for path in train_images_str]\n",
    "        test_images = [Path(path) for path in test_images_str]\n",
    "\n",
    "        return train_images, test_images\n",
    "    \n",
    "    def train(self, train_images, test_images):\n",
    "        train_dataset = DocumentClassificationDataset(train_images, self.processor)\n",
    "        valid_dataset = DocumentClassificationDataset(test_images, self.processor)\n",
    "        \n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=1,\n",
    "            shuffle=True,\n",
    "            #num_workers=10\n",
    "        )\n",
    "        \n",
    "        valid_dataloader = DataLoader(\n",
    "            valid_dataset,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            #num_workers=10\n",
    "        )\n",
    "\n",
    "        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        n_classes = len(self.DOCUMENT_CLASSES)\n",
    "        \n",
    "        model = LayoutLMv3ForSequenceClassification.from_pretrained(\n",
    "                    \"microsoft/layoutlmv3-base\",\n",
    "                    num_labels=n_classes\n",
    "                )\n",
    "        model.to(device)\n",
    "\n",
    "        # load seqeval metric\n",
    "        #metric = evaluate.load(\"seqeval\")\n",
    "        model.config.id2label = {k: v for k, v in enumerate(self.DOCUMENT_CLASSES)}\n",
    "        model.config.label2id = {v: k for k, v in enumerate(self.DOCUMENT_CLASSES)}\n",
    "        # labels of the model\n",
    "        ner_labels = list(model.config.id2label.values())\n",
    "        \n",
    "        num_epochs = 1\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.000001)\n",
    "        \n",
    "        # Initialize an empty DataFrame to store the metrics\n",
    "        columns = [\"Epoch\", \"Training Loss\", \"Validation Loss\", \"Precision\", \"Recall\", \"F1\", \"Accuracy\"]\n",
    "        df_metrics = pd.DataFrame(columns=columns)\n",
    "        \n",
    "        # Early stopping parameters\n",
    "        patience = 3 # Number of epochs to wait for improvement\n",
    "        best_validation_loss = float('inf')\n",
    "        current_patience = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(\"Epoch:\", epoch)\n",
    "        \n",
    "            # Training\n",
    "            model.train()\n",
    "            training_loss = 0.0\n",
    "            num = 0\n",
    "            for batch in tqdm(train_dataloader):\n",
    "                labels = torch.Tensor(batch[\"labels\"]).unsqueeze_(0).long().to(device)\n",
    "                outputs = model(\n",
    "                    input_ids=batch[\"input_ids\"].to(device),\n",
    "                    attention_mask=torch.tensor(batch[\"attention_mask\"]).to(device),\n",
    "                    bbox=torch.tensor(batch[\"bbox\"]).to(device),\n",
    "                    pixel_values=torch.tensor(batch[\"pixel_values\"]).to(device),\n",
    "                    labels=batch[\"labels\"].to(device)\n",
    "                )\n",
    "                loss = outputs.loss\n",
    "                training_loss += loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                num += 1\n",
    "        \n",
    "            print(\"Training Loss:\", training_loss / num)\n",
    "        \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            preds = []\n",
    "            labs = []\n",
    "            validation_loss = 0.0\n",
    "            num = 0\n",
    "            for batch in tqdm(valid_dataloader):\n",
    "                labels = torch.Tensor(batch[\"labels\"]).to(device)\n",
    "                outputs = model(\n",
    "                    input_ids=batch[\"input_ids\"].to(device),\n",
    "                    attention_mask=torch.tensor(batch[\"attention_mask\"]).to(device),\n",
    "                    bbox=torch.tensor(batch[\"bbox\"]).to(device),\n",
    "                    pixel_values=torch.tensor(batch[\"pixel_values\"]).to(device),\n",
    "                    labels=labels\n",
    "                )\n",
    "                loss = outputs.loss\n",
    "                preds_idx = outputs.logits.argmax(axis=1)\n",
    "                labs.append(labels.tolist())\n",
    "                preds.append(preds_idx.tolist())\n",
    "                validation_loss += loss.item()\n",
    "                num += 1\n",
    "        \n",
    "            print(\"Validation Loss:\", validation_loss / num)\n",
    "            print(preds)\n",
    "            print(labs)\n",
    "        \n",
    "            overall_precision, overall_recall, overall_f1, overall_accuracy = compute_metrics([preds, labs])\n",
    "            print(\"Overall Precision:\", overall_precision)\n",
    "            print(\"Overall Recall:\", overall_recall)\n",
    "        \n",
    "            # Store metrics in the DataFrame\n",
    "            metrics_data = {\n",
    "                \"Epoch\": epoch,\n",
    "                \"Training Loss\": training_loss,\n",
    "                \"Validation Loss\": validation_loss,\n",
    "                \"Precision\": overall_precision,\n",
    "                \"Recall\": overall_recall,\n",
    "                \"F1\": overall_f1,\n",
    "                \"Accuracy\": overall_accuracy\n",
    "            }\n",
    "            #df_metrics = df_metrics.append(metrics_data, ignore_index=True)\n",
    "            df_metrics.loc[len(df_metrics)] = metrics_data\n",
    "        \n",
    "            # Early stopping check\n",
    "            if validation_loss < best_validation_loss:\n",
    "                best_validation_loss = validation_loss\n",
    "                current_patience = 0\n",
    "            else:\n",
    "                current_patience += 1\n",
    "                if current_patience >= patience:\n",
    "                    print(f\"Early stopping! No improvement in validation loss for {patience} consecutive epochs.\")\n",
    "                    break\n",
    "        \n",
    "        # Save the DataFrame to a CSV file or do any further analysis\n",
    "        df_metrics.to_csv(\"metrics.csv\", index=False)\n",
    "        print(df_metrics)\n",
    "\n",
    "        return df_metrics\n",
    "        \n",
    "        # Convert DataFrame to markdown\n",
    "        #markdown_table = df_metrics.to_markdown()\n",
    "        \n",
    "        # Print the markdown table\n",
    "        #print(markdown_table)\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "109bb448-6518-4e99-8711-ddde8af686be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-06 12:03:27,659: INFO: common: yaml file: src\\docClassify\\config\\config.yaml loaded successfully]\n",
      "[2024-02-06 12:03:27,665: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-02-06 12:03:27,668: INFO: common: created directory at: artifacts]\n",
      "[2024-02-06 12:03:27,672: INFO: common: created directory at: artifacts/data_training_validation]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kbged\\Miniconda3\\envs\\doc_classify_aws\\Lib\\site-packages\\transformers\\models\\layoutlmv3\\feature_extraction_layoutlmv3.py:30: FutureWarning: The class LayoutLMv3FeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use LayoutLMv3ImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Some weights of LayoutLMv3ForSequenceClassification were not initialized from the model checkpoint at microsoft/layoutlmv3-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b78ff9d999294a72bdb1a14a702397a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kbged\\AppData\\Local\\Temp\\ipykernel_3828\\2638552435.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask=torch.tensor(batch[\"attention_mask\"]).to(device),\n",
      "C:\\Users\\kbged\\AppData\\Local\\Temp\\ipykernel_3828\\2638552435.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bbox=torch.tensor(batch[\"bbox\"]).to(device),\n",
      "C:\\Users\\kbged\\AppData\\Local\\Temp\\ipykernel_3828\\2638552435.py:100: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pixel_values=torch.tensor(batch[\"pixel_values\"]).to(device),\n",
      "C:\\Users\\kbged\\Miniconda3\\envs\\doc_classify_aws\\Lib\\site-packages\\transformers\\modeling_utils.py:907: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.1226830914616586\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "223dd139f1274a8cb2e0b4aa25553dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kbged\\AppData\\Local\\Temp\\ipykernel_3828\\2638552435.py:122: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask=torch.tensor(batch[\"attention_mask\"]).to(device),\n",
      "C:\\Users\\kbged\\AppData\\Local\\Temp\\ipykernel_3828\\2638552435.py:123: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bbox=torch.tensor(batch[\"bbox\"]).to(device),\n",
      "C:\\Users\\kbged\\AppData\\Local\\Temp\\ipykernel_3828\\2638552435.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pixel_values=torch.tensor(batch[\"pixel_values\"]).to(device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.0879846274852754\n",
      "[[0], [0], [2], [2], [2], [2], [2], [0], [2], [2]]\n",
      "[[0], [1], [2], [0], [0], [2], [2], [1], [1], [0]]\n",
      "Overall Precision: 0.26190476190476186\n",
      "Overall Recall: 0.4\n",
      "   Epoch  Training Loss  Validation Loss  Precision  Recall        F1  \\\n",
      "0      0      44.907324        10.879846   0.261905     0.4  0.294286   \n",
      "\n",
      "   Accuracy  \n",
      "0       0.4  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kbged\\Miniconda3\\envs\\doc_classify_aws\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_training_validation_config = config.get_data_training_validation_config()\n",
    "    #data_training_validation = DataTrainingValidationConfig(config=data_training_validation_config)\n",
    "    train_and_validate = TrainAndValidate(data_training_validation_config)\n",
    "    train_images, test_images = train_and_validate.get_train_test_path()\n",
    "    df = train_and_validate.train(train_images, test_images)\n",
    "    #data_training_validation.prepare_all_files()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b67406fa-ff7d-42c2-8ad6-1abc5f53dc91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['balance sheet', 'cashflow', 'income statement']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images\n",
    "sorted(list(map(lambda p: p.name,test_images[0].parent.parent.glob(\"*\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4d507a-07e5-4b10-a664-5973472485f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28b17c8-8c54-4537-bb5d-680829d5d289",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c290c555-c495-42be-bf74-153788ced063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eace4a3-ffac-4e44-9a75-c1c742bf5380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d8e3761-c0ac-47d5-8939-6f176e3e8bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kbged\\Miniconda3\\envs\\doc_classify_aws\\Lib\\site-packages\\transformers\\models\\layoutlmv3\\feature_extraction_layoutlmv3.py:30: FutureWarning: The class LayoutLMv3FeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use LayoutLMv3ImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb9709890634b9285b014fcb1cd37b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kbged\\Miniconda3\\envs\\doc_classify_aws\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kbged\\.cache\\huggingface\\hub\\models--microsoft--layoutlmv3-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edfb9dcd799a40bdbd53fbf0cf72bb52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "341111dfdd3e4b94a0f67f98fe5bd864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.14k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_extractor = LayoutLMv3FeatureExtractor(apply_ocr=False)\n",
    "tokenizer = LayoutLMv3TokenizerFast.from_pretrained(\"microsoft/layoutlmv3-base\")\n",
    "processor = LayoutLMv3Processor(feature_extractor, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fdd948d-83a8-4574-aa3d-b13ddbdcd847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('artifacts/data_ingestion/data/balance sheet/bs7.png'),\n",
       " WindowsPath('artifacts/data_ingestion/data/cashflow/cf9.png'),\n",
       " WindowsPath('artifacts/data_ingestion/data/income statement/is4.png'),\n",
       " WindowsPath('artifacts/data_ingestion/data/balance sheet/bs16.png'),\n",
       " WindowsPath('artifacts/data_ingestion/data/balance sheet/bs15.png'),\n",
       " WindowsPath('artifacts/data_ingestion/data/income statement/is8.png'),\n",
       " WindowsPath('artifacts/data_ingestion/data/income statement/is2.png'),\n",
       " WindowsPath('artifacts/data_ingestion/data/cashflow/cf5.png'),\n",
       " WindowsPath('artifacts/data_ingestion/data/cashflow/cf12.png'),\n",
       " WindowsPath('artifacts/data_ingestion/data/balance sheet/bs3.png')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from pathlib import Path\n",
    "\n",
    "# Convert PosixPath objects to strings\n",
    "image_paths=sorted(list(Path(\"artifacts/data_ingestion/data/\").glob(\"*/*.png\")))\n",
    "image_paths_str = [str(path) for path in image_paths]\n",
    "\n",
    "# Define labels based on whether the paths contain specific strings\n",
    "income_labels = [\"income\" in path for path in image_paths_str]\n",
    "balance_labels = [\"balance\" in path for path in image_paths_str]\n",
    "cashflow_labels = [\"cashflow\" in path for path in image_paths_str]\n",
    "\n",
    "# Use any one of the labels as the target for stratified split\n",
    "# Here, I'm using income_labels, but you can choose based on your requirements\n",
    "train_images_str, test_images_str = train_test_split(image_paths_str, test_size=0.2, stratify=income_labels, random_state=42)\n",
    "\n",
    "# Convert back to PosixPath objects\n",
    "train_images = [Path(path) for path in train_images_str]\n",
    "test_images = [Path(path) for path in test_images_str]\n",
    "\n",
    "DOCUMENT_CLASSES = sorted(list(map(\n",
    "    lambda p: p.name,\n",
    "    Path(\"artifacts/data_ingestion/data/\").glob(\"*\")\n",
    ")))\n",
    "DOCUMENT_CLASSES\n",
    "test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "170f8ed1-1c4e-4130-8e80-e853cf119961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01f1ab35-ebc7-481d-93a7-a2ad9a25dea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DocumentClassificationDataset(train_images, processor)\n",
    "valid_dataset = DocumentClassificationDataset(test_images, processor)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    #num_workers=10\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    #num_workers=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77fbfe3d-841b-451a-9df3-00624f2bc313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feeb207367cb414f9ca209514a38fe6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMv3ForSequenceClassification were not initialized from the model checkpoint at microsoft/layoutlmv3-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LayoutLMv3ForSequenceClassification(\n",
       "  (layoutlmv3): LayoutLMv3Model(\n",
       "    (embeddings): LayoutLMv3TextEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (x_position_embeddings): Embedding(1024, 128)\n",
       "      (y_position_embeddings): Embedding(1024, 128)\n",
       "      (h_position_embeddings): Embedding(1024, 128)\n",
       "      (w_position_embeddings): Embedding(1024, 128)\n",
       "    )\n",
       "    (patch_embed): LayoutLMv3PatchEmbeddings(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (encoder): LayoutLMv3Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_pos_bias): Linear(in_features=32, out_features=12, bias=False)\n",
       "      (rel_pos_x_bias): Linear(in_features=64, out_features=12, bias=False)\n",
       "      (rel_pos_y_bias): Linear(in_features=64, out_features=12, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (classifier): LayoutLMv3ClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LayoutLMv3FeatureExtractor, LayoutLMv3TokenizerFast, LayoutLMv3Processor, LayoutLMv3ForSequenceClassification\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "n_classes = len(DOCUMENT_CLASSES)\n",
    "\n",
    "model = LayoutLMv3ForSequenceClassification.from_pretrained(\n",
    "            \"microsoft/layoutlmv3-base\",\n",
    "            num_labels=n_classes\n",
    "        )\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "166ff97e-4033-43e8-9513-7b54452b4ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2bb9ed9f-6cc3-46b0-bc47-83d7420e2d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1ba75c0ee514ebfad751a917c3134fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kbged\\AppData\\Local\\Temp\\ipykernel_51808\\124497835.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask=torch.tensor(batch[\"attention_mask\"]).to(device),\n",
      "C:\\Users\\kbged\\AppData\\Local\\Temp\\ipykernel_51808\\124497835.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bbox=torch.tensor(batch[\"bbox\"]).to(device),\n",
      "C:\\Users\\kbged\\AppData\\Local\\Temp\\ipykernel_51808\\124497835.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pixel_values=torch.tensor(batch[\"pixel_values\"]).to(device),\n",
      "C:\\Users\\kbged\\Miniconda3\\envs\\doc_classify_aws\\Lib\\site-packages\\transformers\\modeling_utils.py:907: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.086370076239109\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f3bf1ae55042958284e29579dbd28b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kbged\\AppData\\Local\\Temp\\ipykernel_51808\\124497835.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask=torch.tensor(batch[\"attention_mask\"]).to(device),\n",
      "C:\\Users\\kbged\\AppData\\Local\\Temp\\ipykernel_51808\\124497835.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bbox=torch.tensor(batch[\"bbox\"]).to(device),\n",
      "C:\\Users\\kbged\\AppData\\Local\\Temp\\ipykernel_51808\\124497835.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pixel_values=torch.tensor(batch[\"pixel_values\"]).to(device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.083499014377594\n",
      "[[0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
      "[[0], [1], [2], [0], [0], [2], [2], [1], [1], [0]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'compute_metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 79\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(preds)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(labs)\n\u001b[1;32m---> 79\u001b[0m overall_precision, overall_recall, overall_f1, overall_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_metrics\u001b[49m([preds, labs])\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverall Precision:\u001b[39m\u001b[38;5;124m\"\u001b[39m, overall_precision)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverall Recall:\u001b[39m\u001b[38;5;124m\"\u001b[39m, overall_recall)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'compute_metrics' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# load seqeval metric\n",
    "#metric = evaluate.load(\"seqeval\")\n",
    "model.config.id2label = {k: v for k, v in enumerate(DOCUMENT_CLASSES)}\n",
    "model.config.label2id = {v: k for k, v in enumerate(DOCUMENT_CLASSES)}\n",
    "# labels of the model\n",
    "ner_labels = list(model.config.id2label.values())\n",
    "\n",
    "num_epochs = 1\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.000001)\n",
    "\n",
    "# Initialize an empty DataFrame to store the metrics\n",
    "columns = [\"Epoch\", \"Training Loss\", \"Validation Loss\", \"Precision\", \"Recall\", \"F1\", \"Accuracy\"]\n",
    "df_metrics = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 3 # Number of epochs to wait for improvement\n",
    "best_validation_loss = float('inf')\n",
    "current_patience = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch:\", epoch)\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    training_loss = 0.0\n",
    "    num = 0\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        labels = torch.Tensor(batch[\"labels\"]).unsqueeze_(0).long().to(device)\n",
    "        outputs = model(\n",
    "            input_ids=batch[\"input_ids\"].to(device),\n",
    "            attention_mask=torch.tensor(batch[\"attention_mask\"]).to(device),\n",
    "            bbox=torch.tensor(batch[\"bbox\"]).to(device),\n",
    "            pixel_values=torch.tensor(batch[\"pixel_values\"]).to(device),\n",
    "            labels=batch[\"labels\"].to(device)\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        training_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        num += 1\n",
    "\n",
    "    print(\"Training Loss:\", training_loss / num)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labs = []\n",
    "    validation_loss = 0.0\n",
    "    num = 0\n",
    "    for batch in tqdm(valid_dataloader):\n",
    "        labels = torch.Tensor(batch[\"labels\"]).to(device)\n",
    "        outputs = model(\n",
    "            input_ids=batch[\"input_ids\"].to(device),\n",
    "            attention_mask=torch.tensor(batch[\"attention_mask\"]).to(device),\n",
    "            bbox=torch.tensor(batch[\"bbox\"]).to(device),\n",
    "            pixel_values=torch.tensor(batch[\"pixel_values\"]).to(device),\n",
    "            labels=labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        preds_idx = outputs.logits.argmax(axis=1)\n",
    "        labs.append(labels.tolist())\n",
    "        preds.append(preds_idx.tolist())\n",
    "        validation_loss += loss.item()\n",
    "        num += 1\n",
    "\n",
    "    print(\"Validation Loss:\", validation_loss / num)\n",
    "    print(preds)\n",
    "    print(labs)\n",
    "\n",
    "    overall_precision, overall_recall, overall_f1, overall_accuracy = compute_metrics([preds, labs])\n",
    "    print(\"Overall Precision:\", overall_precision)\n",
    "    print(\"Overall Recall:\", overall_recall)\n",
    "\n",
    "    # Store metrics in the DataFrame\n",
    "    metrics_data = {\n",
    "        \"Epoch\": epoch,\n",
    "        \"Training Loss\": training_loss,\n",
    "        \"Validation Loss\": validation_loss,\n",
    "        \"Precision\": overall_precision,\n",
    "        \"Recall\": overall_recall,\n",
    "        \"F1\": overall_f1,\n",
    "        \"Accuracy\": overall_accuracy\n",
    "    }\n",
    "    df_metrics = df_metrics.append(metrics_data, ignore_index=True)\n",
    "\n",
    "    # Early stopping check\n",
    "    if validation_loss < best_validation_loss:\n",
    "        best_validation_loss = validation_loss\n",
    "        current_patience = 0\n",
    "    else:\n",
    "        current_patience += 1\n",
    "        if current_patience >= patience:\n",
    "            print(f\"Early stopping! No improvement in validation loss for {patience} consecutive epochs.\")\n",
    "            break\n",
    "\n",
    "# Save the DataFrame to a CSV file or do any further analysis\n",
    "df_metrics.to_csv(\"metrics.csv\", index=False)\n",
    "df_metrics\n",
    "\n",
    "# Convert DataFrame to markdown\n",
    "markdown_table = df_metrics.to_markdown()\n",
    "\n",
    "# Print the markdown table\n",
    "print(markdown_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1504905b-46be-423e-989e-daed9d01ca93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4119af-8d60-4d64-98b1-c6cc211b34e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fcd00f-9513-41f9-9c56-37ff9ef127ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
